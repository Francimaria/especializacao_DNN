{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de linguagem natural\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de manipualção e visualização de dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Classes do modelo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Funções de avaliação dos modelos\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HatEval** : Este conjunto de dados faz parte da competição SemEval 2019 Task 5 [(Basile et al., 2019)](https://aclanthology.org/S19-2007.pdf) que consiste na detecção de discurso de ódio contra imigrantes e mulheres. Vamos consider apenas a subtarefa A English, um problema de classificação binária para detectar se um tweet em inglês contém discurso de ódio. Mais informações sobre o conjunto de dados HatEval podem ser encontradas em sua página do GitHub: https://github.com/msang/hateval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse dataset já é dividido em treinamento, teste e validação\n",
    "train = pd.read_csv(\"https://raw.githubusercontent.com/Francimaria/especializacao_DNN/main/datasets/hateval/train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/Francimaria/especializacao_DNN/main/datasets/hateval/test.csv\")\n",
    "val = pd.read_csv(\"https://raw.githubusercontent.com/Francimaria/especializacao_DNN/main/datasets/hateval/val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos observar algumas características de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de exemplos em cada conjunto\n",
      "\n",
      " TREINAMENTO: 9000\n",
      "\n",
      " TESTE: 3000\n",
      "\n",
      " VALIDAÇÃO: 1000\n",
      "\n",
      " Distribuição dos dados por classe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validação')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribuição dos dados por \n",
    "print(\"Quantidade de exemplos em cada conjunto\")\n",
    "\n",
    "print(\"\\n TREINAMENTO:\", train.shape[0])\n",
    "\n",
    "print(\"\\n TESTE:\", test.shape[0])\n",
    "\n",
    "print(\"\\n VALIDAÇÃO:\", val.shape[0])\n",
    "\n",
    "print(\"\\n Distribuição dos dados por classe\")\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,5))\n",
    "#percentual \n",
    "sns.barplot(x=\"norm\", y=\"label\",  data=train,ax=ax[0], order=[\"non-hateful\", \"hateful\"],  estimator=lambda x: len(x) / len(train) * 100)\n",
    "ax[0].set_title(\"Treinamento\")\n",
    "sns.barplot(x=\"norm\", y=\"label\", data=test, ax=ax[1], order=[\"non-hateful\", \"hateful\"], estimator=lambda x: len(x) / len(test) * 100)\n",
    "ax[1].set_title(\"Teste\")\n",
    "sns.barplot(x=\"norm\", y=\"label\", data=val, ax=ax[2], order=[\"non-hateful\", \"hateful\"], estimator=lambda x: len(x) / len(val) * 100)\n",
    "ax[2].set_title(\"Validação\")\n",
    "\n",
    "# Se quiser visualizar a contagem \n",
    "# sns.countplot(x=\"label\", data=train, ax=ax[0])\n",
    "# sns.countplot(x=\"label\", data=test, ax=ax[1])\n",
    "# sns.countplot(x=\"label\", data=val, ax=ax[2])\n",
    "\n",
    "# 58%: non-hateful \n",
    "# 42%: hateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos observar os dados**\n",
    "\n",
    "* Features: texto em linguagem natural\n",
    "\n",
    "* Características do texto: obtido a partir de redes sociais (Twitter). Possui alguns elementos como: menções (“i.e.,$@usuario$\"), URLs ( “$http[s]://$\"), RT símbolos, números e etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike</td>\n",
       "      <td>1</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast majority of the ones escaping a war &amp;amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn</td>\n",
       "      <td>1</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe</td>\n",
       "      <td>1</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6</td>\n",
       "      <td>0</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX</td>\n",
       "      <td>0</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0    201   \n",
       "1    202   \n",
       "2    203   \n",
       "3    204   \n",
       "4    205   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0                                                                                                                                                                                     Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike   \n",
       "1  Why would young fighting age men be the vast majority of the ones escaping a war &amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn   \n",
       "2                                               @KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe   \n",
       "3                                                                                                                                                                                NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6   \n",
       "4                                                                                                                                                                                    Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX   \n",
       "\n",
       "   label         norm  \n",
       "0      1      hateful  \n",
       "1      1      hateful  \n",
       "2      1      hateful  \n",
       "3      0  non-hateful  \n",
       "4      0  non-hateful  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# primeiramente vamos observar os dados\n",
    "with pd.option_context('display.max_colwidth',None):\n",
    "  display(train.head(5))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pré-processamento\n",
    "\n",
    "A etapa de pré-processamento é responsável por preparar os dados para a etapa de extração de características. \n",
    "\n",
    "Nessa etapa podem ser realizadas diferentes tarefas como:  \n",
    "\n",
    "* Limpeza dos dados: remoção de menções (“i.e.,$@usuario$\"), URLs ( “$http[s]://$\"), RT símbolos, números, stop-words e espaços em branco redundantes.\n",
    "\n",
    "* Normalização dos dados: conversão do texto completo para minúsculo e normalização de unicode [link](https://www.ibm.com/docs/en/db2-for-zos/11?topic=ccsids-normalization-unicode-strings).\n",
    "\n",
    "\n",
    "Verificar a biblioteca [NLTK](https://www.nltk.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/npi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk                                             # Natural Language Toolkit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re                                               # biblioteca para operações com regular expression\n",
    "from nltk.corpus import stopwords                       # módulo para tratamento das stop words que vem com o NLTK\n",
    "import string                                           # para operações com string\n",
    "from unicodedata import normalize                       # para normalização de unicodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"wouldn't\", 'not', 'were', 'against', 'myself', 'ourselves', 'through', 'shan', 'ain', 'out', 'd', 'your', 'has', 'which', 'both', 'for', 're', 'once', 'off', 'does', 'when', 'all', \"she's\", 'until', 'itself', 'because', 'hadn', 'up', 'some', 'it', 'herself', 'will', 'yourself', \"couldn't\", \"mightn't\", 'hasn', 'weren', 'again', 'was', 'of', 'whom', 'here', 'doesn', \"weren't\", \"won't\", 'who', \"didn't\", \"hadn't\", 'been', 'hers', 'below', 'needn', 'what', 'the', 'nor', 'why', 'isn', 'and', 've', \"isn't\", 'don', 'i', 'me', 'such', 'same', 'mightn', 'are', 'you', \"it's\", 'too', 'her', 'themselves', 'shouldn', 'our', 'very', 'aren', 'a', 'their', 'with', 'before', 'as', 'did', \"shouldn't\", \"needn't\", 'above', 'doing', \"should've\", 'o', 'ours', 'while', \"you'll\", 'where', 'each', \"don't\", 'by', 'him', 'himself', 'few', 'other', 's', 'more', 'only', 'have', 'll', \"you've\", 'but', 'into', 'be', 'from', 'being', 'didn', \"hasn't\", 'then', \"mustn't\", 'if', 'after', 'can', 'an', 'yourselves', 'couldn', 'any', 'my', 'ma', 'wouldn', 'its', 'had', \"that'll\", 'than', 'those', 'm', 'how', 'do', \"doesn't\", \"haven't\", 'should', 'now', 'during', 'further', 'own', 'having', 'between', 'them', 'we', 'mustn', 'so', 'wasn', 'at', 'am', 'they', 'just', 'no', 'under', 't', \"aren't\", 'or', 'this', 'theirs', 'is', 'there', 'to', 'on', \"wasn't\", 'over', 'haven', 'yours', 'his', 'he', 'these', 'down', 'in', 'won', \"you'd\", 'about', \"shan't\", 'she', 'y', 'most', \"you're\", 'that'}\n",
      "Quantidade:  179\n",
      "{'haja', 'hei', 'tiver', 'tiverem', 'você', 'meus', 'pelas', 'às', 'aquilo', 'seu', 'fôssemos', 'eram', 'nem', 'tua', 'houvemos', 'e', 'nosso', 'for', 'até', 'fomos', 'haver', 'houverem', 'mesmo', 'qual', 'pela', 'com', 'delas', 'estava', 'lhe', 'nossos', 'estejam', 'sem', 'esses', 'terei', 'pelo', 'teve', 'muito', 'ela', 'vos', 'houveram', 'houvéssemos', 'dos', 'houvessem', 'houvéramos', 'tivessem', 'esteve', 'tu', 'houver', 'teu', 'vocês', 'estivera', 'seria', 'foi', 'será', 'estamos', 'tenho', 'estão', 'teria', 'deles', 'lhes', 'tem', 'tenham', 'ser', 'como', 'esse', 'estas', 'estivessem', 'estivemos', 'há', 'um', 'estivéssemos', 'isto', 'aos', 'seríamos', 'houveria', 'são', 'quem', 'me', 'nos', 'terão', 'minha', 'teríamos', 'sejam', 'te', 'fora', 'seriam', 'tivermos', 'tinham', 'teriam', 'ao', 'a', 'tínhamos', 'aquelas', 'forem', 'ou', 'das', 'em', 'tivera', 'sou', 'as', 'depois', 'tiveram', 'era', 'houve', 'serão', 'é', 'este', 'dela', 'mais', 'tivesse', 'hajamos', 'o', 'hajam', 'está', 'aquele', 'estive', 'fosse', 'de', 'tenhamos', 'terá', 'fôramos', 'eu', 'houveríamos', 'mas', 'eles', 'estiverem', 'estou', 'só', 'para', 'tive', 'se', 'éramos', 'minhas', 'também', 'hão', 'tivéssemos', 'quando', 'esta', 'fui', 'estivéramos', 'numa', 'tuas', 'dele', 'que', 'aqueles', 'houveremos', 'houverá', 'uma', 'estar', 'houvermos', 'nas', 'sua', 'já', 'seja', 'meu', 'somos', 'tinha', 'estávamos', 'pelos', 'do', 'tivemos', 'nossa', 'elas', 'teus', 'aquela', 'essas', 'estavam', 'houveriam', 'esteja', 'houvera', 'os', 'teremos', 'estes', 'nossas', 'tém', 'temos', 'nós', 'essa', 'à', 'no', 'estejamos', 'isso', 'havemos', 'estivermos', 'sejamos', 'num', 'na', 'suas', 'formos', 'não', 'serei', 'tenha', 'da', 'fossem', 'por', 'houverão', 'entre', 'tivéramos', 'foram', 'estivesse', 'seus', 'seremos', 'ele', 'estiveram', 'estiver', 'houverei', 'houvesse'}\n",
      "Quantidade:  207\n",
      "{'tengamos', 'estuviste', 'qué', 'sentida', 'habrá', 'habíais', 'estuvierais', 'esas', 'nosotras', 'antes', 'vuestros', 'tienes', 'tiene', 'del', 'nuestras', 'estuviéramos', 'lo', 'estaba', 'estaremos', 'tu', 'hay', 'estuvieran', 'tuviste', 'tuvisteis', 'tenidas', 'con', 'quienes', 'desde', 'teníais', 'como', 'tenga', 'estabais', 'estando', 'su', 'estuvieron', 'habríamos', 'me', 'tendrán', 'vuestras', 'estos', 'tenemos', 'hubiste', 'hasta', 'nada', 'estéis', 'estaríamos', 'tuviésemos', 'fuese', 'estoy', 'eres', 'otras', 'está', 'algunas', 'tus', 'seré', 'eras', 'estaría', 'tendrían', 'ni', 'les', 'otros', 'mí', 'ya', 'cual', 'serán', 'sobre', 'estabas', 'suyas', 'hubiese', 'estuviesen', 'por', 'entre', 'estuviera', 'habías', 'habido', 'tenéis', 'tenidos', 'hubiéramos', 'estuvieseis', 'estaréis', 'haya', 'habremos', 'tuyos', 'habréis', 'tuyas', 'al', 'habíamos', 'algunos', 'habría', 'estará', 'fueses', 'esa', 'estamos', 'mía', 'mis', 'ellas', 'seríamos', 'hubo', 'otro', 'seáis', 'tuya', 'fuésemos', 'fue', 'habidos', 'esos', 'estuve', 'era', 'más', 'estuvisteis', 'muchos', 'tendré', 'tengo', 'mi', 'serían', 'erais', 'seamos', 'sentidos', 'estarías', 'nuestros', 'ante', 'han', 'estemos', 'mucho', 'tuviesen', 'fui', 'tuvieras', 'sin', 'uno', 'las', 'tienen', 'hayas', 'habiendo', 'somos', 'son', 'ellos', 'suyos', 'muy', 'le', 'os', 'el', 'la', 'no', 'hubieses', 'vosotros', 'estaban', 'habrían', 'tendríais', 'tendrías', 'donde', 'mías', 'hubiésemos', 'fuisteis', 'ella', 'fueseis', 'algo', 'sentido', 'tuviéramos', 'habida', 'hubiera', 'nosotros', 'todos', 'quien', 'míos', 'has', 'sean', 'fueran', 'hubiesen', 'habrás', 'estarán', 'hayamos', 'hubieseis', 'habían', 'estuviésemos', 'teniendo', 'tendríamos', 'será', 'tengan', 'tenía', 'teníamos', 'tendrás', 'sintiendo', 'fuiste', 'estarían', 'también', 'hubierais', 'fuesen', 'tuviera', 'tuvimos', 'pero', 'te', 'hayáis', 'estábamos', 'estado', 'contra', 'tuviese', 'sí', 'es', 'de', 'vuestro', 'seréis', 'tuvieseis', 'una', 'tenías', 'éramos', 'tú', 'fuimos', 'esta', 'que', 'estar', 'tendría', 'serás', 'estaré', 'tenido', 'esté', 'sería', 'poco', 'sois', 'fuera', 'estuvieras', 'sus', 'estén', 'habré', 'un', 'yo', 'estad', 'estuviese', 'otra', 'estarás', 'tuvieran', 'estadas', 'hemos', 'hubimos', 'habidas', 'fueron', 'e', 'tengas', 'había', 'tuvieses', 'mío', 'tuvo', 'tengáis', 'eso', 'sentidas', 'tanto', 'habríais', 'todo', 'estas', 'habrías', 'estaríais', 'sea', 'estés', 'estuvimos', 'esto', 'nos', 'porque', 'fueras', 'suyo', 'a', 'seas', 'siente', 'tenida', 'en', 'él', 'este', 'sentid', 'o', 'estás', 'hubieran', 'ha', 'tendrá', 'ti', 'unos', 'están', 'suya', 'estuvieses', 'tendremos', 'habéis', 'para', 'se', 'tened', 'hube', 'durante', 'ese', 'eran', 'hubieron', 'tuyo', 'tenían', 'nuestro', 'tuvieron', 'tendréis', 'nuestra', 'seríais', 'vuestra', 'hayan', 'estada', 'hubisteis', 'los', 'habrán', 'tuve', 'serías', 'estuvo', 'fuerais', 'soy', 'cuando', 'vosotras', 'he', 'hubieras', 'fuéramos', 'seremos', 'tuvierais', 'estados', 'y', 'estáis'}\n",
      "Quantidade:  313\n"
     ]
    }
   ],
   "source": [
    "# se quiser visualizar a lista de stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)\n",
    "print(\"Quantidade: \",len(stops))\n",
    "# os idiomas também podem ser usados \n",
    "stops = set(stopwords.words('portuguese'))\n",
    "print(stops)\n",
    "print(\"Quantidade: \",len(stops))\n",
    "stops = set(stopwords.words('spanish'))\n",
    "print(stops)\n",
    "print(\"Quantidade: \",len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# vamos checar alguns exemplos\n",
    "# Diferentes unicodes para gerar a mesma letra\n",
    "print(\"\\u00C7\", \"\\u0043\\u0327\")\n",
    "\n",
    "print(\"Ç\" == \"Ç\")\n",
    "\n",
    "print(\"Ç\"==\"Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_replace(text):\n",
    "    \"\"\"\n",
    "    1) Substituição de muitos espaços em branco por uma instância\n",
    "    2) Remoção de RT, urls, menções, números\n",
    "    3) Normalização de unicode\n",
    "\n",
    "    Retorne o texto pré-processado\n",
    "    \"\"\"\n",
    "\n",
    "    # padrão muitos espaços em branco\n",
    "    space_pattern = '\\s+'\n",
    "\n",
    "    # padrão de RT\n",
    "    rt_regex = 'RT @[\\w_]+'    \n",
    "\n",
    "    # padrão para urls\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # padrão para menções\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "\n",
    "    #padrão para números\n",
    "    number_regex = '[0-9]'\n",
    "\n",
    "    # Removendo espaços extras\n",
    "    parsed_text = re.sub(space_pattern, ' ', text)\n",
    "    \n",
    "    #Removendo RT\n",
    "    parsed_text = re.sub(rt_regex, ' ', parsed_text) \n",
    "\n",
    "    # Removendo URLs    \n",
    "    parsed_text = re.sub(giant_url_regex, ' ', parsed_text)\n",
    "\n",
    "    # Removendo menções\n",
    "    parsed_text = re.sub(mention_regex, ' ', parsed_text)\n",
    "\n",
    "    # Removendo números\n",
    "    parsed_text = re.sub(number_regex, ' ', parsed_text)\n",
    "\n",
    "    # Normalização de unicode\n",
    "    parsed_text = normalize('NFKD', parsed_text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "  \"\"\" Transforme o texto em minúsculo\"\"\"\n",
    "  return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "\t\"\"\" Use NLTK para remover todas as stop-words em inglês \"\"\"\n",
    "\tstop_words = set(stopwords.words('english'))  #portuguese\n",
    "\n",
    "\treturn ' '.join([word for word in text.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(tweets):\t\n",
    "\tclean_tweets = []\n",
    "\n",
    "\tfor text in tweets:\n",
    "\t\t#Limpando o texto\n",
    "\t\ttext = clear_replace(str(text))\n",
    "\t\t#Conversão do texto completo para minúsculo\n",
    "\t\ttext = to_lower(text)\n",
    "\t\t# Removendo stopwords\n",
    "\t\ttext = remove_stopwords(text)\n",
    "\t\tclean_tweets.append(text)\n",
    "\treturn clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quem te falou isso?', 'ela ama trabalhar com nlp dias por semana apenas']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos verificar um exemplo. Vou colocar em português, mas para o HatEval dataset as stopwords devem ser em inglês. \n",
    "\n",
    "exemplo_1 = \"RT @chiquinho Quem te falou       ISSO?\"\n",
    "\n",
    "exemplo_2 = \"ELA ama trabalhar com NLP 5 dias por semana https://www.nltk.org/, apenas\"\n",
    "\n",
    "tweets = [exemplo_1 , exemplo_2]\n",
    "\n",
    "pre_processing(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "300\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Vamos avaliar apenas um subset dos conjuntos de treinamento, teste e validação com 10% dos exemplos\n",
    "train_sub, _, _,_ = train_test_split(train, train[\"label\"], train_size=0.1, stratify=train[\"label\"])\n",
    "test_sub, _, _,_ = train_test_split(test, test[\"label\"], train_size=0.1, stratify=test[\"label\"])\n",
    "val_sub, _, _,_ = train_test_split(val, val[\"label\"], train_size=0.1, stratify=val[\"label\"])\n",
    "\n",
    "print(train_sub.shape[0])\n",
    "print(test_sub.shape[0])\n",
    "print(val_sub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2931                                                                                            *some* malaysians still look chinese &amp; indian friends immigrant. can't fanthom really.\n",
       "3054    worth keep eye reactions refugees already islands included change. camps like #moria, lesvos, extremely volatile protests/riots happened smaller issues past. expect sort reaction\n",
       "911                                                                               white house wants make harder legal migrants citizens via good!!! best!! welfare leeches go back home!!!\n",
       "8435                                                                                                                                                                    white women stupid\n",
       "7856                                                                                   wow life institute use really sexist language (hysterical woman!!!). women speaking means rantin...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sub[\"text\"] = pre_processing(train_sub[\"text\"])\n",
    "test_sub[\"text\"] = pre_processing(test_sub[\"text\"])\n",
    "val_sub[\"text\"] = pre_processing(val_sub[\"text\"])\n",
    "\n",
    "_X_train, y_train = train_sub[\"text\"], train_sub[\"label\"]\n",
    "_X_test, y_test = test_sub[\"text\"], test_sub[\"label\"]\n",
    "_X_val, y_val = val_sub[\"text\"], val_sub[\"label\"]\n",
    "\n",
    "# primeiramente vamos observar os dados\n",
    "with pd.option_context('display.max_colwidth',None):\n",
    "  display(_X_train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de características\n",
    "\n",
    "Como representar uma palavra? \n",
    "\n",
    "Em PLN os conjuntos de dados geralmente estão disponíveis como texto bruto para análise. Portanto, a extração de características visa transformar o texto em linguagem natural em um espaço vetorial numérico adequado como entradas do modelo. Várias técnicas de extração de características podem ser aplicadas, como técnicas de Bag-of-Words (BoW), recursos lexicais e métodos de embedding.\n",
    "\n",
    "\n",
    "Vamos utilizar o TF-IDF (frequency–inverse document frequency). O TF-IDF é uma medida estatística que tem o intuito de indicar a importância de uma palavra de um documento em relação a uma coleção de documentos ou em um corpus linguístico.\n",
    "\n",
    "$$TFIDF (t, d, D) = tf (t, d) \\times idf (t, D)$$\n",
    "\n",
    "tf - frequência absoluta\n",
    "\n",
    "idf - A frequência inversa do documento é uma medida de quanta informação a palavra fornece, ou seja, se é comum ou rara em todos os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(_X_train)\n",
    "X_test = vectorizer.transform(_X_test)\n",
    "X_val = vectorizer.transform(_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 4225)\n",
      "  (0, 3000)\t0.23696555177984835\n",
      "  (0, 1311)\t0.3467559050502096\n",
      "  (0, 535)\t0.2682672981613883\n",
      "  (0, 1839)\t0.1726779574472497\n",
      "  (0, 1467)\t0.26361923862621145\n",
      "  (0, 1885)\t0.3467559050502096\n",
      "  (0, 148)\t0.23208471035436448\n",
      "  (0, 618)\t0.3467559050502096\n",
      "  (0, 2224)\t0.24238977167841086\n",
      "  (0, 3541)\t0.24238977167841086\n",
      "  (0, 2275)\t0.3467559050502096\n",
      "  (0, 3452)\t0.3467559050502096\n",
      "  (1, 2992)\t0.22634957986063323\n",
      "  (1, 3464)\t0.21344209251978088\n",
      "  (1, 1257)\t0.21344209251978088\n",
      "  (1, 2700)\t0.18221858915630718\n",
      "  (1, 1976)\t0.20428408450847022\n",
      "  (1, 3416)\t0.22634957986063323\n",
      "  (1, 1628)\t0.19718058162700242\n",
      "  (1, 3143)\t0.21344209251978088\n",
      "  (1, 2901)\t0.19718058162700242\n",
      "  (1, 3996)\t0.21344209251978088\n",
      "  (1, 1279)\t0.22634957986063323\n",
      "  (1, 2164)\t0.22634957986063323\n",
      "  (1, 2453)\t0.22634957986063323\n",
      "  :\t:\n",
      "  (897, 3591)\t0.5465693242331372\n",
      "  (897, 2329)\t0.4666141057844456\n",
      "  (897, 989)\t0.38415044391079084\n",
      "  (898, 1801)\t0.49685097397666356\n",
      "  (898, 3987)\t0.49685097397666356\n",
      "  (898, 3391)\t0.34331536304606186\n",
      "  (898, 1035)\t0.4484158813922197\n",
      "  (898, 3950)\t0.4328232643108167\n",
      "  (899, 3764)\t0.24934590738223245\n",
      "  (899, 1523)\t0.24934590738223245\n",
      "  (899, 983)\t0.24934590738223245\n",
      "  (899, 119)\t0.24934590738223245\n",
      "  (899, 2968)\t0.24934590738223245\n",
      "  (899, 2134)\t0.24934590738223245\n",
      "  (899, 3684)\t0.47025412873022404\n",
      "  (899, 46)\t0.23512706436511202\n",
      "  (899, 3081)\t0.2108197916773909\n",
      "  (899, 4203)\t0.22503863469451132\n",
      "  (899, 588)\t0.23512706436511202\n",
      "  (899, 634)\t0.21721344070624565\n",
      "  (899, 1803)\t0.18370558277662416\n",
      "  (899, 3138)\t0.1929061680185245\n",
      "  (899, 4157)\t0.20073136200679018\n",
      "  (899, 2533)\t0.16220524630194869\n",
      "  (899, 1839)\t0.12416959987562576\n"
     ]
    }
   ],
   "source": [
    "#vamos verificar o tamanho do vetor\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_x' 'a_x' 'aaaawwww' ... 'yzy' 'zone' 'zsef']\n",
      "Número de features: 4225\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Número de features:\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "## com o pré-processamento: 4153\n",
    "## sem o pré-processamento: 5182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar agora um modelo pré-treinado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: zeugma in /home/npi/.local/lib/python3.10/site-packages (0.49)\n",
      "Requirement already satisfied: Cython>=0.27.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (3.0.8)\n",
      "Requirement already satisfied: gensim>=3.5.0 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (4.3.2)\n",
      "Requirement already satisfied: keras>=2.1.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (1.22.2)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (1.2.1)\n",
      "Requirement already satisfied: tensorflow>=1.5.0 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (2.10.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/npi/.local/lib/python3.10/site-packages (from gensim>=3.5.0->zeugma) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/npi/.local/lib/python3.10/site-packages (from gensim>=3.5.0->zeugma) (6.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/npi/.local/lib/python3.10/site-packages (from pandas>=0.20.3->zeugma) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/npi/.local/lib/python3.10/site-packages (from pandas>=0.20.3->zeugma) (2021.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/npi/.local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->zeugma) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/npi/.local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->zeugma) (3.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.60.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (21.3)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow>=1.5.0->zeugma)\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow>=1.5.0->zeugma) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow>=1.5.0->zeugma) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (0.36.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (4.0.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow>=1.5.0->zeugma) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/npi/.local/lib/python3.10/site-packages (from packaging->tensorflow>=1.5.0->zeugma) (3.0.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/npi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/npi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/npi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/npi/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/npi/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/npi/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (3.2.0)\n",
      "Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.1\n",
      "    Uninstalling protobuf-4.21.1:\n",
      "      Successfully uninstalled protobuf-4.21.1\n",
      "Successfully installed protobuf-3.19.6\n"
     ]
    }
   ],
   "source": [
    "!pip install zeugma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "fastText = EmbeddingTransformer('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "X_train_f = fastText.fit_transform(_X_train)\n",
    "X_test_f = fastText.transform(_X_test)\n",
    "X_val_f = fastText.transform(_X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo\n",
    "\n",
    " Vamos avaliar o modelo usando a regressão logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.29      0.39       174\n",
      "           1       0.43      0.73      0.54       126\n",
      "\n",
      "    accuracy                           0.48       300\n",
      "   macro avg       0.51      0.51      0.47       300\n",
      "weighted avg       0.53      0.48      0.46       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "#treinando o modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predição\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Resultados do classificador\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 0 - non-hateful\n",
    "# 1 - hateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E se ajustarmos os parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor parametro: {'penalty': 'none'}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "#penalty - depende do algoritmo usado para otimização (solver) padrão: ‘lbfgs’ - [‘l2’, None]\n",
    "# 'none': nenhuma penalidade é adicionada;\n",
    "# 'l2': adiciona um termo de penalidade L2 e é a escolha padrão;\n",
    "\n",
    "parameters = {'penalty': ['L2', 'none']}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,             \n",
    "                    param_grid = parameters,     \n",
    "                    scoring = 'f1_macro',          # métrica de avaliação\n",
    "                    cv = 5)                        # cross-validation\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "print(\"Melhor parametro:\", grid.best_params_)         \n",
    "# e ver a sua performance no dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.28      0.40       174\n",
      "           1       0.45      0.80      0.57       126\n",
      "\n",
      "    accuracy                           0.50       300\n",
      "   macro avg       0.55      0.54      0.48       300\n",
      "weighted avg       0.57      0.50      0.47       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Treinando o modelo final com o parâmetro selecionado \n",
    "model = LogisticRegression(penalty='none', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predição\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Resultados do classificador\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E se quisermos gerar um modelo que faça a extração e a classificação sequencialmente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2931    *some* malaysians still look chinese &amp; ind...\n",
       "3054    worth keep eye reactions refugees already isla...\n",
       "911     white house wants make harder legal migrants c...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = train_sub[\"text\"], train_sub[\"label\"]\n",
    "X_test, y_test = test_sub[\"text\"], test_sub[\"label\"]\n",
    "X_val, y_val = val_sub[\"text\"], val_sub[\"label\"]\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: [('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(penalty='none', random_state=42))]\n",
      "Score: 0.500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('tfidf', TfidfVectorizer()), \n",
    "                 ('lr', LogisticRegression(penalty='none', random_state=42))])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Steps:\", pipe.steps)\n",
    "print(\"Score: %.3f\" %pipe.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
