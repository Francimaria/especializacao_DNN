{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de linguagem natural\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de manipualção e visualização de dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Classes do modelo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Funções de avaliação dos modelos\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HatEval** : Este conjunto de dados faz parte da competição SemEval 2019 Task 5 [(Basile et al., 2019)](https://aclanthology.org/S19-2007.pdf) que consiste na detecção de discurso de ódio contra imigrantes e mulheres. Vamos consider apenas a subtarefa A English, um problema de classificação binária para detectar se um tweet em inglês contém discurso de ódio. Mais informações sobre o conjunto de dados HatEval podem ser encontradas em sua página do GitHub: https://github.com/msang/hateval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse dataset já é dividido em treinamento, teste e validação\n",
    "train = pd.read_csv(\"https://raw.githubusercontent.com/Francimaria/especializacao_DNN/main/datasets/hateval/train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/Francimaria/especializacao_DNN/main/datasets/hateval/test.csv\")\n",
    "val = pd.read_csv(\"https://raw.githubusercontent.com/Francimaria/especializacao_DNN/main/datasets/hateval/val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos observar algumas características de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de exemplos em cada conjunto\n",
      "\n",
      " TREINAMENTO: 9000\n",
      "\n",
      " TESTE: 3000\n",
      "\n",
      " VALIDAÇÃO: 1000\n",
      "\n",
      " Distribuição dos dados por classe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validação')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribuição dos dados por \n",
    "print(\"Quantidade de exemplos em cada conjunto\")\n",
    "\n",
    "print(\"\\n TREINAMENTO:\", train.shape[0])\n",
    "\n",
    "print(\"\\n TESTE:\", test.shape[0])\n",
    "\n",
    "print(\"\\n VALIDAÇÃO:\", val.shape[0])\n",
    "\n",
    "print(\"\\n Distribuição dos dados por classe\")\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,5))\n",
    "#percentual \n",
    "sns.barplot(x=\"norm\", y=\"label\",  data=train,ax=ax[0], order=[\"non-hateful\", \"hateful\"],  estimator=lambda x: len(x) / len(train) * 100)\n",
    "ax[0].set_title(\"Treinamento\")\n",
    "sns.barplot(x=\"norm\", y=\"label\", data=test, ax=ax[1], order=[\"non-hateful\", \"hateful\"], estimator=lambda x: len(x) / len(test) * 100)\n",
    "ax[1].set_title(\"Teste\")\n",
    "sns.barplot(x=\"norm\", y=\"label\", data=val, ax=ax[2], order=[\"non-hateful\", \"hateful\"], estimator=lambda x: len(x) / len(val) * 100)\n",
    "ax[2].set_title(\"Validação\")\n",
    "\n",
    "# Se quiser visualizar a contagem \n",
    "# sns.countplot(x=\"label\", data=train, ax=ax[0])\n",
    "# sns.countplot(x=\"label\", data=test, ax=ax[1])\n",
    "# sns.countplot(x=\"label\", data=val, ax=ax[2])\n",
    "\n",
    "# 58%: non-hateful \n",
    "# 42%: hateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos observar os dados**\n",
    "\n",
    "* Features: texto em linguagem natural\n",
    "\n",
    "* Características do texto: obtido a partir de redes sociais (Twitter). Possui alguns elementos como: menções (“i.e.,$@usuario$\"), URLs ( “$http[s]://$\"), RT símbolos, números e etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike</td>\n",
       "      <td>1</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast majority of the ones escaping a war &amp;amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn</td>\n",
       "      <td>1</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe</td>\n",
       "      <td>1</td>\n",
       "      <td>hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6</td>\n",
       "      <td>0</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX</td>\n",
       "      <td>0</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0    201   \n",
       "1    202   \n",
       "2    203   \n",
       "3    204   \n",
       "4    205   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0                                                                                                                                                                                     Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike   \n",
       "1  Why would young fighting age men be the vast majority of the ones escaping a war &amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn   \n",
       "2                                               @KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe   \n",
       "3                                                                                                                                                                                NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6   \n",
       "4                                                                                                                                                                                    Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX   \n",
       "\n",
       "   label         norm  \n",
       "0      1      hateful  \n",
       "1      1      hateful  \n",
       "2      1      hateful  \n",
       "3      0  non-hateful  \n",
       "4      0  non-hateful  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# primeiramente vamos observar os dados\n",
    "with pd.option_context('display.max_colwidth',None):\n",
    "  display(train.head(5))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pré-processamento\n",
    "\n",
    "A etapa de pré-processamento é responsável por preparar os dados para a etapa de extração de características. \n",
    "\n",
    "Nessa etapa podem ser realizadas diferentes tarefas como:  \n",
    "\n",
    "* Limpeza dos dados: remoção de menções (“i.e.,$@usuario$\"), URLs ( “$http[s]://$\"), RT símbolos, números, stop-words e espaços em branco redundantes.\n",
    "\n",
    "* Normalização dos dados: conversão do texto completo para minúsculo e normalização de unicode [link](https://www.ibm.com/docs/en/db2-for-zos/11?topic=ccsids-normalization-unicode-strings).\n",
    "\n",
    "\n",
    "Verificar a biblioteca [NLTK](https://www.nltk.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/npi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk                                             # Natural Language Toolkit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re                                               # biblioteca para operações com regular expression\n",
    "from nltk.corpus import stopwords                       # módulo para tratamento das stop words que vem com o NLTK\n",
    "import string                                           # para operações com string\n",
    "from unicodedata import normalize                       # para normalização de unicodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", 'why', 'our', 'himself', 'having', 'll', 'some', 'for', 'about', 'd', 'ain', \"aren't\", 'don', 'because', 'you', 'only', 'both', 'theirs', 'have', 'when', 'mustn', 'weren', 'won', 'me', 'm', \"mustn't\", 'off', 'his', 'ourselves', 'here', 'this', 'can', 'who', 'whom', 'from', 'further', 'o', 'myself', 'yours', 'a', 'the', 'my', 'at', 'more', \"couldn't\", 'he', 'haven', 'they', 'i', \"it's\", \"won't\", 'if', 'between', 'not', 'which', 'few', 'your', \"that'll\", 'down', \"should've\", 'an', 'while', 'ma', 'before', 'hadn', 'up', 'shouldn', 'their', 'its', 'to', 'each', 'by', 're', 'during', 's', \"doesn't\", 'any', 'doesn', 'doing', 'again', 'aren', \"haven't\", 'than', 'same', 'above', 'couldn', 'all', 'other', \"don't\", 'as', 'am', 'hasn', 've', 'these', \"you've\", 'been', 'out', 'so', 'nor', 'that', 'had', \"weren't\", 'against', 'below', 'isn', 'do', 'them', 'in', 'just', \"you'd\", 'being', \"wouldn't\", 'is', 'wasn', \"hasn't\", 'until', \"didn't\", 'most', 'has', 'of', 't', 'were', 'then', 'y', 'too', 'through', \"mightn't\", 'she', 'it', 'did', 'but', 'herself', 'under', \"she's\", 'after', 'be', \"wasn't\", 'her', 'and', 'mightn', 'over', 'are', 'how', \"you're\", 'there', 'yourself', 'yourselves', \"shan't\", 'didn', \"you'll\", 'hers', 'him', 'we', 'needn', 'such', 'ours', 'itself', 'own', 'no', 'on', \"isn't\", 'with', 'will', 'does', 'once', 'what', 'those', 'now', 'wouldn', 'was', 'where', 'shan', 'into', 'should', \"hadn't\", 'themselves', 'very', \"shouldn't\", 'or'}\n",
      "Quantidade:  179\n",
      "{'forem', 'deles', 'elas', 'estejamos', 'te', 'terão', 'vocês', 'sejam', 'dele', 'for', 'estamos', 'houverem', 'sejamos', 'mais', 'estas', 'minha', 'ao', 'seremos', 'pela', 'sou', 'me', 'este', 'nosso', 'houvemos', 'fossem', 'se', 'foi', 'estivéramos', 'tenho', 'tivemos', 'lhe', 'num', 'uma', 'estiverem', 'lhes', 'mesmo', 'quem', 'o', 'tiveram', 'tuas', 'era', 'a', 'terá', 'houver', 'isso', 'hajamos', 'numa', 'não', 'tinham', 'às', 'tivermos', 'que', 'estejam', 'nossas', 'ela', 'havemos', 'houverei', 'estavam', 'houvéramos', 'tu', 'aquelas', 'aos', 'são', 'estou', 'foram', 'você', 'houveríamos', 'tínhamos', 'hajam', 'tiverem', 'ou', 'tenhamos', 'estivéssemos', 'éramos', 'dos', 'houvermos', 'fomos', 'nossos', 'os', 'minhas', 'estivera', 'formos', 'também', 'nem', 'nos', 'teríamos', 'tive', 'fôssemos', 'houvessem', 'meus', 'fora', 'entre', 'seja', 'isto', 'suas', 'as', 'com', 'tivessem', 'ele', 'houveram', 'aquela', 'serei', 'para', 'só', 'teu', 'em', 'seu', 'um', 'eram', 'há', 'estava', 'seria', 'teus', 'do', 'muito', 'tivéssemos', 'tinha', 'delas', 'já', 'aqueles', 'pelos', 'hei', 'serão', 'seriam', 'será', 'por', 'nós', 'estivesse', 'eu', 'tem', 'essas', 'estão', 'houvera', 'teria', 'esse', 'nossa', 'das', 'esteve', 'de', 'é', 'meu', 'estivermos', 'aquilo', 'depois', 'da', 'houveremos', 'tém', 'estivemos', 'temos', 'houveria', 'estávamos', 'houvesse', 'hão', 'tivera', 'sem', 'tenha', 'essa', 'tenham', 'como', 'aquele', 'houverá', 'haja', 'dela', 'tivesse', 'esteja', 'teremos', 'estes', 'mas', 'pelas', 'e', 'tiver', 'houve', 'quando', 'fui', 'fôramos', 'esses', 'haver', 'terei', 'vos', 'no', 'qual', 'sua', 'está', 'pelo', 'somos', 'à', 'fosse', 'houvéssemos', 'seríamos', 'ser', 'tua', 'estivessem', 'estiveram', 'houveriam', 'teriam', 'esta', 'houverão', 'teve', 'eles', 'estive', 'estiver', 'tivéramos', 'até', 'na', 'nas', 'estar', 'seus'}\n",
      "Quantidade:  207\n",
      "{'mi', 'hubieseis', 'hubiste', 'cuando', 'tuvimos', 'te', 'esto', 'hayas', 'suyo', 'habrían', 'estas', 'seré', 'seas', 'tuviste', 'estuvimos', 'también', 'más', 'estuviste', 'los', 'desde', 'estando', 'estáis', 'tus', 'estés', 'habían', 'nuestros', 'nos', 'hubieses', 'había', 'teníamos', 'para', 'durante', 'tengo', 'ni', 'estuvieses', 'esas', 'hubo', 'soy', 'tú', 'todo', 'suya', 'siente', 'de', 'hubieran', 'contra', 'sí', 'sentido', 'yo', 'habré', 'habías', 'fueses', 'tuviera', 'estaréis', 'fuéramos', 'estás', 'un', 'serían', 'sin', 'fuera', 'tuyos', 'tendrían', 'habidas', 'tenidos', 'tenga', 'estén', 'está', 'habrás', 'somos', 'sois', 'en', 'seríais', 'ti', 'porque', 'esta', 'estaban', 'quienes', 'otras', 'tengas', 'estuvisteis', 'donde', 'tuve', 'estar', 'tuviéramos', 'tuvieran', 'estuvieras', 'estamos', 'estuvieron', 'estado', 'serán', 'me', 'este', 'vuestro', 'eran', 'mío', 'tenían', 'era', 'tuvo', 'ella', 'ese', 'uno', 'serás', 'tuviésemos', 'ha', 'habidos', 'estuvierais', 'tu', 'ellos', 'sentid', 'mía', 'hay', 'estaríamos', 'han', 'tenías', 'tengamos', 'sentida', 'estaríais', 'estuviesen', 'fueron', 'sus', 'habíamos', 'tuyas', 'os', 'otro', 'habríais', 'habríamos', 'tendrías', 'estuve', 'fuese', 'tanto', 'habíais', 'tiene', 'quien', 'hayáis', 'habrías', 'mucho', 'estarías', 'será', 'por', 'la', 'estabas', 'nada', 'tendrá', 'tuviese', 'míos', 'como', 'habría', 'haya', 'fuésemos', 'hubimos', 'vuestros', 'tendré', 'tengáis', 'tenía', 'tendría', 'nuestras', 'fui', 'algunos', 'estoy', 'fuisteis', 'seríamos', 'sea', 'hubiesen', 'es', 'tuvisteis', 'hayamos', 'teníais', 'habido', 'le', 'estaremos', 'vuestras', 'eras', 'seremos', 'antes', 'fuerais', 'hubieras', 'estemos', 'se', 'con', 'serías', 'habiendo', 'o', 'tienen', 'tendréis', 'a', 'fuiste', 'tenidas', 'he', 'fue', 'que', 'al', 'pero', 'algunas', 'habremos', 'el', 'las', 'tuvieras', 'otra', 'él', 'estuviésemos', 'estéis', 'estadas', 'habrán', 'mis', 'mías', 'tuvieron', 'fuimos', 'tenido', 'sobre', 'estuvieran', 'estuvieseis', 'seréis', 'estuviera', 'sería', 'suyas', 'estada', 'hasta', 'estos', 'tendríais', 'unos', 'están', 'muy', 'tendríamos', 'tuviesen', 'estados', 'algo', 'hubisteis', 'lo', 'hubiéramos', 'estaría', 'tuvierais', 'nuestro', 'tuvieses', 'estábamos', 'esa', 'seamos', 'hubiésemos', 'estad', 'una', 'cual', 'vosotras', 'teniendo', 'esté', 'todos', 'poco', 'tienes', 'tendremos', 'les', 'tuvieseis', 'sentidos', 'fueran', 'tuyo', 'estabais', 'hubiera', 'estarán', 'otros', 'eso', 'mí', 'hube', 'seáis', 'ya', 'estarás', 'éramos', 'estará', 'habréis', 'hubierais', 'tenida', 'hubieron', 'hubiese', 'tenéis', 'esos', 'estaré', 'tuya', 'entre', 'tengan', 'ante', 'vosotros', 'habida', 'tenemos', 'su', 'tendrás', 'habrá', 'suyos', 'muchos', 'habéis', 'qué', 'fueras', 'ellas', 'hayan', 'vuestra', 'has', 'del', 'y', 'sintiendo', 'tendrán', 'estuviese', 'estuvo', 'erais', 'estaba', 'son', 'nosotras', 'fuesen', 'e', 'tened', 'nosotros', 'fueseis', 'no', 'sean', 'eres', 'nuestra', 'estuviéramos', 'hemos', 'sentidas', 'estarían'}\n",
      "Quantidade:  313\n"
     ]
    }
   ],
   "source": [
    "# se quiser visualizar a lista de stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)\n",
    "print(\"Quantidade: \",len(stops))\n",
    "# os idiomas também podem ser usados \n",
    "stops = set(stopwords.words('portuguese'))\n",
    "print(stops)\n",
    "print(\"Quantidade: \",len(stops))\n",
    "stops = set(stopwords.words('spanish'))\n",
    "print(stops)\n",
    "print(\"Quantidade: \",len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ç Ç\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# vamos checar alguns exemplos\n",
    "# Diferentes unicodes para gerar a mesma letra\n",
    "print(\"\\u00C7\", \"\\u0043\\u0327\")\n",
    "\n",
    "print(\"Ç\" == \"Ç\")\n",
    "\n",
    "print(\"Ç\"==\"Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_replace(text):\n",
    "    \"\"\"\n",
    "    1) Substituição de muitos espaços em branco por uma instância\n",
    "    2) Remoção de RT, urls, menções, números\n",
    "    3) Normalização de unicode\n",
    "\n",
    "    Retorne o texto pré-processado\n",
    "    \"\"\"\n",
    "\n",
    "    # padrão muitos espaços em branco\n",
    "    space_pattern = '\\s+'\n",
    "\n",
    "    # padrão de RT\n",
    "    rt_regex = 'RT @[\\w_]+'    \n",
    "\n",
    "    # padrão para urls\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # padrão para menções\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "\n",
    "    #padrão para números\n",
    "    number_regex = '[0-9]'\n",
    "\n",
    "    # Removendo espaços extras\n",
    "    parsed_text = re.sub(space_pattern, ' ', text)\n",
    "    \n",
    "    #Removendo RT\n",
    "    parsed_text = re.sub(rt_regex, ' ', parsed_text) \n",
    "\n",
    "    # Removendo URLs    \n",
    "    parsed_text = re.sub(giant_url_regex, ' ', parsed_text)\n",
    "\n",
    "    # Removendo menções\n",
    "    parsed_text = re.sub(mention_regex, ' ', parsed_text)\n",
    "\n",
    "    # Removendo números\n",
    "    parsed_text = re.sub(number_regex, ' ', parsed_text)\n",
    "\n",
    "    # Normalização de unicode\n",
    "    parsed_text = normalize('NFKD', parsed_text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "  \"\"\" Transforme o texto em minúsculo\"\"\"\n",
    "  return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "\t\"\"\" Use NLTK para remover todas as stop-words em inglês \"\"\"\n",
    "\tstop_words = set(stopwords.words('english'))  #portuguese\n",
    "\n",
    "\treturn ' '.join([word for word in text.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(tweets):\t\n",
    "\tclean_tweets = []\n",
    "\n",
    "\tfor text in tweets:\n",
    "\t\t#Limpando o texto\n",
    "\t\ttext = clear_replace(str(text))\n",
    "\t\t#Conversão do texto completo para minúsculo\n",
    "\t\ttext = to_lower(text)\n",
    "\t\t# Removendo stopwords\n",
    "\t\ttext = remove_stopwords(text)\n",
    "\t\tclean_tweets.append(text)\n",
    "\treturn clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quem te falou isso?', 'ela ama trabalhar com nlp dias por semana apenas']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos verificar um exemplo. Vou colocar em português, mas para o HatEval dataset as stopwords devem ser em inglês. \n",
    "\n",
    "exemplo_1 = \"RT @chiquinho Quem te falou       ISSO?\"\n",
    "\n",
    "exemplo_2 = \"ELA ama trabalhar com NLP 5 dias por semana https://www.nltk.org/, apenas\"\n",
    "\n",
    "tweets = [exemplo_1 , exemplo_2]\n",
    "\n",
    "pre_processing(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "300\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Vamos avaliar apenas um subset dos conjuntos de treinamento, teste e validação com 10% dos exemplos\n",
    "train_sub, _, _,_ = train_test_split(train, train[\"label\"], train_size=0.1, stratify=train[\"label\"], random_state=42)\n",
    "test_sub, _, _,_ = train_test_split(test, test[\"label\"], train_size=0.1, stratify=test[\"label\"], random_state=42)\n",
    "val_sub, _, _,_ = train_test_split(val, val[\"label\"], train_size=0.1, stratify=val[\"label\"], random_state=42)\n",
    "\n",
    "print(train_sub.shape[0])\n",
    "print(test_sub.shape[0])\n",
    "print(val_sub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788                                                                      heart trump #trumptrain #buildthatwall\n",
       "6394                              bri: put hole- me: *starts laughing like child* bri: *starts laughing* whoops\n",
       "1717    wait turn...guyanese immigrant explains ny post agrees us president, donald trump's immigration policy.\n",
       "4511                                     ok fucking said leave block me. dm first i'm gonna kick ass. shut fuck\n",
       "6541                          back. even crowley. 'cause everything done, owe us, son bitch. get ass here, make\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sub[\"text\"] = pre_processing(train_sub[\"text\"])\n",
    "test_sub[\"text\"] = pre_processing(test_sub[\"text\"])\n",
    "val_sub[\"text\"] = pre_processing(val_sub[\"text\"])\n",
    "\n",
    "_X_train, y_train = train_sub[\"text\"], train_sub[\"label\"]\n",
    "_X_test, y_test = test_sub[\"text\"], test_sub[\"label\"]\n",
    "_X_val, y_val = val_sub[\"text\"], val_sub[\"label\"]\n",
    "\n",
    "# primeiramente vamos observar os dados\n",
    "with pd.option_context('display.max_colwidth',None):\n",
    "  display(_X_train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de características\n",
    "\n",
    "Como representar uma palavra? \n",
    "\n",
    "Em PLN os conjuntos de dados geralmente estão disponíveis como texto bruto para análise. Portanto, a extração de características visa transformar o texto em linguagem natural em um espaço vetorial numérico adequado como entradas do modelo. Várias técnicas de extração de características podem ser aplicadas, como técnicas de Bag-of-Words (BoW), recursos lexicais e métodos de embedding.\n",
    "\n",
    "\n",
    "Vamos utilizar o TF-IDF (frequency–inverse document frequency). O TF-IDF é uma medida estatística que tem o intuito de indicar a importância de uma palavra de um documento em relação a uma coleção de documentos ou em um corpus linguístico.\n",
    "\n",
    "$$TFIDF (t, d, D) = tf (t, d) \\times idf (t, D)$$\n",
    "\n",
    "tf - frequência absoluta\n",
    "\n",
    "idf - A frequência inversa do documento é uma medida de quanta informação a palavra fornece, ou seja, se é comum ou rara em todos os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(_X_train)\n",
    "X_test = vectorizer.transform(_X_test)\n",
    "X_val = vectorizer.transform(_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 4083)\n"
     ]
    }
   ],
   "source": [
    "#vamos verificar o tamanho do vetor\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2ythats' 'a_x' 'aap' ... 'zeebrugge' 'zero' 'zone']\n",
      "Número de features: 4083\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Número de features:\", len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "## com o pré-processamento: 4153\n",
    "## sem o pré-processamento: 5182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar agora um modelo pré-treinado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: zeugma in /home/npi/.local/lib/python3.10/site-packages (0.49)\n",
      "Requirement already satisfied: Cython>=0.27.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (3.0.8)\n",
      "Requirement already satisfied: gensim>=3.5.0 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (4.3.2)\n",
      "Requirement already satisfied: keras>=2.1.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (1.22.2)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (1.2.1)\n",
      "Requirement already satisfied: tensorflow>=1.5.0 in /home/npi/.local/lib/python3.10/site-packages (from zeugma) (2.10.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/npi/.local/lib/python3.10/site-packages (from gensim>=3.5.0->zeugma) (1.9.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/npi/.local/lib/python3.10/site-packages (from gensim>=3.5.0->zeugma) (6.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/npi/.local/lib/python3.10/site-packages (from pandas>=0.20.3->zeugma) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/npi/.local/lib/python3.10/site-packages (from pandas>=0.20.3->zeugma) (2021.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/npi/.local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->zeugma) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/npi/.local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->zeugma) (3.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.60.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (21.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow>=1.5.0->zeugma) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow>=1.5.0->zeugma) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (0.36.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (4.0.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorflow>=1.5.0->zeugma) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow>=1.5.0->zeugma) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/npi/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/npi/.local/lib/python3.10/site-packages (from packaging->tensorflow>=1.5.0->zeugma) (3.0.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/npi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/npi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/npi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/npi/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/npi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/npi/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/npi/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=1.5.0->zeugma) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install zeugma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 12:59:26.764929: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 12:59:26.864548: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-23 12:59:26.868267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-23 12:59:26.868280: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-23 12:59:26.887998: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-23 12:59:27.278619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-23 12:59:27.278665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-23 12:59:27.278669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "fastText = EmbeddingTransformer('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "X_train_f = fastText.fit_transform(_X_train)\n",
    "X_test_f = fastText.transform(_X_test)\n",
    "X_val_f = fastText.transform(_X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo\n",
    "\n",
    " Vamos avaliar o modelo usando a regressão logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.24      0.32       174\n",
      "           1       0.40      0.69      0.50       126\n",
      "\n",
      "    accuracy                           0.43       300\n",
      "   macro avg       0.45      0.46      0.41       300\n",
      "weighted avg       0.46      0.43      0.40       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "#treinando o modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predição\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Resultados do classificador\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 0 - non-hateful\n",
    "# 1 - hateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E se ajustarmos os parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor parametro: {'penalty': 'none'}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "#penalty - depende do algoritmo usado para otimização (solver) padrão: ‘lbfgs’ - [‘l2’, None]\n",
    "# 'none': nenhuma penalidade é adicionada;\n",
    "# 'l2': adiciona um termo de penalidade L2 e é a escolha padrão;\n",
    "\n",
    "parameters = {'penalty': ['L2', 'none']}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,             \n",
    "                    param_grid = parameters,     \n",
    "                    scoring = 'f1_macro',          # métrica de avaliação\n",
    "                    cv = 5)                        # cross-validation\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "print(\"Melhor parametro:\", grid.best_params_)         \n",
    "# e ver a sua performance no dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.21      0.30       174\n",
      "           1       0.42      0.78      0.54       126\n",
      "\n",
      "    accuracy                           0.45       300\n",
      "   macro avg       0.49      0.49      0.42       300\n",
      "weighted avg       0.50      0.45      0.40       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Treinando o modelo final com o parâmetro selecionado \n",
    "model = LogisticRegression(penalty='none', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predição\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Resultados do classificador\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E se quisermos gerar um modelo que faça a extração e a classificação sequencialmente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788                heart trump #trumptrain #buildthatwall\n",
       "6394    bri: put hole- me: *starts laughing like child...\n",
       "1717    wait turn...guyanese immigrant explains ny pos...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = train_sub[\"text\"], train_sub[\"label\"]\n",
    "X_test, y_test = test_sub[\"text\"], test_sub[\"label\"]\n",
    "X_val, y_val = val_sub[\"text\"], val_sub[\"label\"]\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos salvando os resultados em um dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos importar algumas métricas\n",
    "\n",
    "from sklearn.metrics import (auc, accuracy_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>acuracia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [modelo, acuracia]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos usar um dataFrame para ir salvando os resultados\n",
    "# adicione outras métricas \n",
    "df_results = pd.DataFrame(columns=['modelo', 'acuracia'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>acuracia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelo 1</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     modelo acuracia\n",
       "0  modelo 1     0.43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('tfidf', TfidfVectorizer()), \n",
    "                 ('lr', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "#round value\n",
    "acc = round(acc, 2)\n",
    "\n",
    "#f_score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Adicione o resultado no dataframe \n",
    "# criando uma nova linha para adicionar no dataFrame\n",
    "new_row = {'modelo': 'modelo 1','acuracia': acc}\n",
    "df_results = df_results.append(new_row, ignore_index=True)\n",
    "print(\"Acurácia: %.3f\" %acc)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.427\n",
      "Melhor parametro: {'lr__penalty': 'none'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>acuracia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelo 1</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modelo 2</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     modelo acuracia\n",
       "0  modelo 1     0.43\n",
       "1  modelo 2     0.45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modelo 2\n",
    "\n",
    "# ajustando os parâmetros \n",
    "model =  Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                  ('lr', LogisticRegression(random_state=42))])\n",
    "\n",
    "# Nos parâmetros precisamos especificar a qual etapa ele pertence (nome__) * Obs.: são dois underlines (__)\n",
    "parameters = {'lr__penalty': ['L2', 'none']}\n",
    "\n",
    "grid = GridSearchCV(estimator = model,             # k-nn\n",
    "                    param_grid = parameters,       # dicionário com valores para serem testados.\n",
    "                    scoring = 'accuracy',          # métrica de avaliação #f1_macro\n",
    "                    cv = 5)                        # cross-validation\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "#round value\n",
    "acc = round(acc, 2)\n",
    "\n",
    "#f_score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Adicione o resultado no dataframe \n",
    "# criando uma nova linha para adicionar no dataFrame\n",
    "new_row = {'modelo': 'modelo 2','acuracia': acc}\n",
    "df_results = df_results.append(new_row, ignore_index=True)\n",
    "print(\"Acurácia: %.3f\" %pipe.score(X_test, y_test))\n",
    "\n",
    "print(\"Melhor parametro:\", grid.best_params_)     \n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avalie o modelo com outros classificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que tal testar o mlp?\n",
    "\n",
    "Obs.: Lembre que precisamos realizar a extração de características \n",
    "\n",
    "Use o random_state=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 3\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Use um pipeline \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha um dos modelo visto em aula para classificação \n",
    "\n",
    "**Use o random_state=42 e o tfidf como extrator de características**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
